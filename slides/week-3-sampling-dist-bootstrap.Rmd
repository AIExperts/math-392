---
title: "Week Three"
output: ioslides_presentation
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Install these packages first
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
library(readr)
library(knitr)
```

# Problem Set 2 Review

## {.smaller}

Find the distribution of the p-value when $H_0$ is true.

```{r}
n <- 20
it <- 500
obs <- rep(NA, it)
pvals <- rep(NA, it)
x <- rep(1:2, c(n/2, n/2))

for(j in 1:it) {
  y <- rnorm(n)
  obs[j] <- mean(y[x == 1]) - mean(y[x == 2])
  perm_stats <- rep(NA, it)
  for(i in 1:it) {
    x_perm <- sample(x)
    perm_stats[i] <- mean(y[x_perm == 1]) - mean(y[x_perm == 2])
  }
  
  pvals[j] <- (sum(perm_stats < obs[j]) + 1) / (it + 1)
}
```

## {.smaller}

```{r fig.height = 3.5}
df <- data.frame(perm_stats, pvals)
ggplot(df, aes(x = perm_stats)) +
  geom_histogram()
```

## {.smaller}

```{r fig.height = 3.5}
ggplot(df, aes(x = pvals)) +
  geom_histogram()
```

# Sampling distributions

## Example 1

```{r}
n <- 200
it <- 500
obs <- rep(NA, it)
x <- rep(1:2, c(n/2, n/2))

for(j in 1:it) {
  y <- rnorm(n)
  obs[j] <- mean(y[x == 1]) - mean(y[x == 2])
}
```

##

```{r}
df <- data.frame(obs)
ggplot(df, aes(x = obs)) +
  geom_density()
```


## Example 2

```{r}
n <- 12
it <- 500
obs <- rep(NA, it)

for(j in 1:it) {
  y <- runif(n)
  obs[j] <- max(y)
}
```

## {.build}

```{r}
fn <- function(x)
df <- data.frame(obs)
ggplot(df, aes(x = obs)) +
  geom_density(lwd = 2) +
  stat_function(fun = function(x) {12 * x ^ 11}, 
                col = "tomato", lwd = 2)
```


# Large sample approximation

## Average of Gammas

```{r}
n <- 17
it <- 1000
r <- 100
lambda <- 5
obs <- rep(NA, it)

for(j in 1:it) {
  x <- rgamma(n, shape = r, rate = lambda)
  obs[j] <- mean(x)
}
```

##

```{r}
df <- data.frame(x = c(obs, (obs - 20)/(2/sqrt(17))),
                 type = rep(c("x", "z"), times = c(it, it)))
ggplot(df, aes(x = x, col = type)) +
  geom_density() +
  geom_vline(xintercept = 2.45) +
  geom_vline(xintercept = 21.9)
```

##

```{r}
df %>%
  filter(type == "x") %>%
  ggplot(aes(x = x)) +
  geom_density(col = 3) +
  stat_function(fun = dnorm, args = list(mean = 20, sd = 2/sqrt(17)))
```

## CLT for Binomial

```{r}
n <- 30
p <- .5
it <- 5000
obs <- rep(NA, it)

for(j in 1:it) {
  x <- rbinom(1, size = n, p)
  obs[j] <- x
}
```

##

The problem with discrete distributions.

```{r echo = FALSE}
tab <- table(obs)
df <- data.frame(x = as.integer(names(tab)),
                 frac = tab/sum(tab))
```

```{r warning=FALSE}
ggplot(df, aes(x = x, y = frac.Freq)) +
  geom_histogram(stat = "identity") +
  stat_function(fun = dnorm, args = list(mean = n*p, sd = sqrt(n*p*(1-p))))
```


## CLT for another Binomial

```{r}
n <- 30
p <- .2
it <- 5000
obs <- rep(NA, it)

for(j in 1:it) {
  x <- rbinom(1, size = n, p)
  obs[j] <- x
}
```

##

The problem with skewed distributions.

```{r echo = FALSE}
tab <- table(obs)
df <- data.frame(x = as.integer(names(tab)),
                 frac = tab/sum(tab))
```

```{r warning=FALSE}
ggplot(df, aes(x = x, y = frac.Freq)) +
  geom_histogram(stat = "identity") +
  stat_function(fun = dnorm, args = list(mean = n*p, sd = sqrt(n*p*(1-p))))
```


## Lessons from the binomial

The accuracy of the Normal approximation depends upon:

- $n$
- Whether $X$ is discrete (use "continuity correction")
- The skew of $X$



# The Bootstrap

## Ex: return to the binomial

Let $X \sim \textrm{Binom}(n = 30, p = .2)$. Here's the simulation:

```{r}
for (j in 1:it) {
  x <- rbinom(1, size = n, p)
  obs[j] <- x
}
```

Here's the bootstrap from a single sample:

```{r}
x <- rbinom(n, 1, p)
boot_stat <- rep(NA, it)

for (j in 1:it) {
  boot <- sample(x, size = n, replace = T)
  boot_stat[j] <- sum(boot)
}
```


# Bias

## Bias: case study {.build}

A major study of the association between blood pressure and cardiovascular disease found:

- 55 out of 3338 men with high bp died of cardiovascular disease ($\hat{p}_1 = 0.0165$)
- 21 out of 2676 men with low bp died of cardiovascular disease ($\hat{p}_2 = 0.0078$)

We are interested in estimating the *relative risk* with an estimator:

$$
RR = \theta = \frac{P(\textrm{death} \,| \,\textrm{high bp})}{P(\textrm{death}\, | \,\textrm{low bp})}
$$
$$
\hat{\theta} = \frac{\hat{p}_1}{\hat{p}_2} = \frac{0.0165}{0.0078} = 2.12
$$


## Two sample bootstrap

Procedure:

1. Draw sample of size $n_1 = 3338$ with replacement from high bp group and separate sample of size $n_2 = 2676$ from low bp group.
2. For each pair of samples, compute $\hat{p}_1$ and $\hat{p}_2$.
3. Compute $\hat{\theta} = \frac{\hat{p}_1}{\hat{p}_2}$.
4. Repeat 1 - 3 many times and consider distribution of $\hat{\theta}$.


## Two sample bootstrap

```{r}
n1 <- 3338
n2 <- 2676
g1 <- rep(c(1, 0), c(55, n1 - 55))
g2 <- rep(c(1, 0), c(21, n1 - 21))
it <- 5000
boot_RR<- rep(NA, it)

for (i in 1:it) {
  b1 <- sample(g1, size = n1, replace = TRUE)
  b2 <- sample(g2, size = n2, replace = TRUE)
  boot_RR[i] <- mean(b1)/mean(b2)
}
```

## Bootstrap distribution

```{r echo = FALSE}
df <- data.frame(boot_RR)
```

```{r}
ggplot(df, aes(x = boot_RR)) +
  geom_histogram(fill = "white", col = "steelblue") +
  geom_vline(xintercept = 2.12, col = "tomato") +
  geom_vline(xintercept = mean(boot_RR), col = "green")
```


## Bias

```{r}
bias <- mean(boot_RR) - 2.12
bias
se_boot <- sd(boot_RR)
bias/se_boot
```

