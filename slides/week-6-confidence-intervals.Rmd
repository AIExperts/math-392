---
title: "Week Six"
output: ioslides_presentation
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Install these packages first
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
library(readr)
library(knitr)
```


# Exam Review

## Simulation approach{.build}

Set some population parameters.

```{r}
p1 <- .016
p2 <- .005
theta <- p1/p2
theta
```


## Simulation approach {.build}

Take a single sample. 

```{r}
set.seed(384)
n1 <- 3338
n2 <- 2676
g1 <- sample(c(1, 0), size = n1, 
             prob = c(p1, 1 - p1), replace = TRUE)
g2 <- sample(c(1, 0), size = n2,
             prob = c(p2, 1 - p2), replace = TRUE)
theta_hat <- mean(g1)/mean(g2)
theta_hat
```


## Simulation approach {.build}

Make a single bias estimate via bootstrap.

```{r}
it <- 5000
boot_RR<- rep(NA, it)
for (i in 1:it) {
  b1 <- sample(g1, size = n1, replace = TRUE)
  b2 <- sample(g2, size = n2, replace = TRUE)
  boot_RR[i] <- mean(b1)/mean(b2)
}
bias_hat <- mean(boot_RR) - theta_hat
bias_hat
```



## {.build .smaller}

```{r}
library(tidyverse)
df <- data.frame(boot_RR)
ggplot(df, aes(x = boot_RR)) +
  geom_histogram(fill = "white", col = "darkgray") +
  geom_vline(xintercept = theta_hat, col = "tomato") +
  geom_vline(xintercept = mean(boot_RR), col = "darkgreen") +
  geom_vline(xintercept = theta, col = "purple")
```


## True bias {.build .smaller}

In the sim setting, we can find the true bias:

$$
E(\hat{\theta}) - \theta
$$

Because we can learn (approximately) the first term via repeated simulation.

```{r}
theta_hats <- rep(NA, it)
for(i in 1:it) {
  g1 <- sample(c(1, 0), size = n1, 
             prob = c(p1, 1 - p1), replace = TRUE)
  g2 <- sample(c(1, 0), size = n2,
             prob = c(p2, 1 - p2), replace = TRUE)
  theta_hats[i] <- mean(g1)/mean(g2)
}
bias <- mean(theta_hats[i]) - theta 
bias
```


## Sampling dist of $\widehat{bias}_{boot}$ {.build}

The sampling distribution will of this statistic will tell us both its bias and its variance.

To make dreams inside of dreams we need to:

1. Generate a sample from the true parameters.
2. Build up a bootstrap distribution from sample.
3. Calculate a single $\widehat{bias}_{boot} = E(\hat{\theta}^*) - \hat{\theta}$
4. Repeat steps 1 - 3 to get a full distribution of $\widehat{bias}_{boot}$.


## Sampling dist of $\widehat{bias}_{boot}$ {.build}

```{r cache = TRUE}
bias_hats_a <- rep(NA, it)
it <- 500
for (i in 1:it) {
  g1 <- sample(c(1, 0), size = n1, 
             prob = c(p1, 1 - p1), replace = TRUE)
  g2 <- sample(c(1, 0), size = n2,
             prob = c(p2, 1 - p2), replace = TRUE)
  theta_hat <- mean(g1)/mean(g2)
  boot_RR<- rep(NA, it)
  for (j in 1:it) {
    b1 <- sample(g1, size = n1, replace = TRUE)
    b2 <- sample(g2, size = n2, replace = TRUE)
    boot_RR[j] <- mean(b1)/mean(b2)
  }
  bias_hats_a[i] <- mean(boot_RR) - theta_hat
}
```


## {.build .smaller}

```{r warning=FALSE}
bias_hats_a[bias_hats_a == Inf] <- NA
df <- data.frame(bias_hats_a)
ggplot(df, aes(x = bias_hats_a)) +
  geom_histogram(fill = "white", col = "darkgray") +
  geom_vline(xintercept = mean(bias_hats_a, na.rm = TRUE), col = "darkgreen") +
  geom_vline(xintercept = bias, col = "purple")
```


## Double Bootstrap

```{r cache = TRUE}
bias_hats_b <- rep(NA, it)
for (i in 1:it) {
  g1_boot <- sample(g1, size = n1, replace = TRUE) #first boot
  g2_boot <- sample(g2, size = n2, replace = TRUE)
  theta_hat <- mean(g1_boot)/mean(g2_boot)
  boot_RR <- rep(NA, it)
  for (j in 1:it) {
    g1_boot_b <- sample(g1_boot, size = n1, replace = TRUE) #double boot!
    g2_boot_b <- sample(g2_boot, size = n1, replace = TRUE)
    boot_RR[j] <- mean(g1_boot_b)/mean(g2_boot_b)
  }
  bias_hats_b[i] <- mean(boot_RR) - theta_hat
}
```


## {.build .smaller}

```{r warning=FALSE}
bias_hats_b[bias_hats_b == Inf] <- NA
df <- data.frame(bias_hats_b)
ggplot(df, aes(x = bias_hats_b)) +
  geom_histogram(fill = "white", col = "darkgray") +
  geom_vline(xintercept = mean(bias_hats_b, na.rm = TRUE), col = "darkgreen") +
  geom_vline(xintercept = bias, col = "purple")
```


## Biases compared {.build}

```{r}
mean(bias_hats_a, na.rm = TRUE) - bias
mean(bias_hats_b, na.rm = TRUE) - bias
```

## Variances compared {.build}

```{r}
sd(bias_hats_a, na.rm = TRUE)
sd(bias_hats_b, na.rm = TRUE)
```


# Confidence Intervals

## Unknown sigma

```{r eval = FALSE}
it <- 10000
w <- rep(NA, it)
n <- 15
for(i in 1:it) {
  x <- rnorm(n, 25, 7)
  xbar <- mean(x)
  s <- sd(x)
  w[i] <- (xbar - 25) / (s/sqrt(n))
}
qqnorm(w, pch = ".")
abline(0, 1)
```


## Unknown sigma

```{r echo = FALSE}
it <- 10000
w <- rep(NA, it)
n <- 15
for(i in 1:it) {
  x <- rnorm(n, 25, 7)
  xbar <- mean(x)
  s <- sd(x)
  w[i] <- (xbar - 25) / (s/sqrt(n))
}
qqnorm(w, pch = ".")
abline(0, 1)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Set seed for random number generator
set.seed(76)
```



## t Distribution

Degrees of freedom = 1

```{r, echo=FALSE}
curve(dnorm(x, mean=0, sd=1), from=-4, to=4, n=1000, xlab="t", ylab="Density f(t)", 
      col="white", lty=2)
curve(dt(x, df=1), from=-4, to=4, n=1000, col="black", add=TRUE)
legend(
  "topright",
  bty="n",
  legend = c("t with df=1"),
  col = c("black"),
  lty = c(1)
)
```



## t Distribution

Degrees of freedom = 3

```{r, echo=FALSE}
curve(dnorm(x, mean=0, sd=1), from=-4, to=4, n=1000, xlab="t", ylab="Density f(t)", 
      col="white", lty=2)
curve(dt(x, df=3), from=-4, to=4, n=1000, col="red", add=TRUE)
curve(dt(x, df=1), from=-4, to=4, n=1000, col="black", add=TRUE)
legend(
  "topright",
  bty="n",
  legend = c("t with df=1", "t with df=3"),
  col = c("black", "red"),
  lty = c(1, 1)
)
```



## t Distribution

Degrees of freedom = 7

```{r, echo=FALSE}
curve(dnorm(x, mean=0, sd=1), from=-4, to=4, n=1000, xlab="t", ylab="Density f(t)", 
      col="white", lty=2)
curve(dt(x, df=7), from=-4, to=4, n=1000, col="green", add=TRUE)
curve(dt(x, df=3), from=-4, to=4, n=1000, col="red", add=TRUE)
curve(dt(x, df=1), from=-4, to=4, n=1000, col="black", add=TRUE)
legend(
  "topright",
  bty="n",
  legend = c("t with df=1", "t with df=3", "t with df=7"),
  col = c("black", "red", "green"),
  lty = c(1, 1, 1)
)
```



## t Distribution

At Degrees of freedom = $\infty$, the $t$ becomes a $Z \sim N(0,1)$

```{r, echo=FALSE}
curve(dnorm(x, mean=0, sd=1), from=-4, to=4, n=1000, xlab="t", ylab="Density f(t)", 
      col="blue", lty=2)
curve(dt(x, df=7), from=-4, to=4, n=1000, col="green", add=TRUE)
curve(dt(x, df=3), from=-4, to=4, n=1000, col="red", add=TRUE)
curve(dt(x, df=1), from=-4, to=4, n=1000, col="black", add=TRUE)
legend(
  "topright",
  bty="n",
  legend = c("t with df=1", "t with df=3", "t with df=7", "z"),
  col = c("black", "red", "green", "blue"),
  lty = c(1, 1, 1, 2)
)
```


## Baby weights {.build}

Find a 99% CI for the mean weight of baby girls born in North Carolina in 2004 (p. 175).

```{r, warning=FALSE, message=FALSE, fig.width=4, fig.height=4}
library(resampledata)
data(NCBirths2004)
girls <- NCBirths2004 %>%
  filter(Gender == "Female") %>%
  select(Weight) %>%
  pull()
```



## Baby weights, cont. {.build}

Step 1: is $X$ normal?

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.width=8, fig.height=4}
par(mfrow=c(1,2))
hist(girls, xlab="weight (in grams)", main="NC Girls Weights")
qqnorm(girls)
qqline(girls)
```



## Baby weights, cont. {.build}

Form the 95% interval from the statistics.

```{r}
x_bar <- mean(girls)
s <- sd(girls)
n <- length(girls)
alpha <- .05
q1 <- qt(alpha/2, df = n - 1)
q2 <- qt(1 - alpha/2, df = n - 1)
LB <- x_bar - q2 * s / sqrt(n)
UB <- x_bar - q1 * s / sqrt(n)
c(LB, UB)
```



## Baby weights, cont. {.build}

Alternatively, the canned version.

```{r}
t.test(girls, conf.level=0.95)
```


# Approximate Intervals


## Skew vs Sample Size

Say we have the following skewed population distribution and we are interested in
$\overline{x}$:

```{r, echo=FALSE}
meanlog <- -5
sdlog <- 1
x.max <- 0.04
breaks <- seq(0, 5, by=0.0005)

x <- seq(0, x.max, by=0.0001)
plot(x, dlnorm(x, meanlog = meanlog, sdlog=sdlog), xlab="x", ylab="",
     xlim=c(0, x.max), type='l')
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 2
sim.means <- replicate(10000, mean(rlnorm(n, meanlog = meanlog, sdlog=sdlog)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 1300))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 10
sim.means <- replicate(10000, mean(rlnorm(n, meanlog = meanlog, sdlog=sdlog)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 1300))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 30
sim.means <- replicate(10000, mean(rlnorm(n, meanlog = meanlog, sdlog=sdlog)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 1300))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 75
sim.means <- replicate(10000, mean(rlnorm(n, meanlog = meanlog, sdlog=sdlog)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 1300))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```



## Skew vs Sample Size

Now say we have the following population distribution and we are interested in
the behavior of $\overline{x}$ for sample size $n$.

```{r, echo=FALSE}
x <- seq(0, x.max, by=0.0001)
plot(x, dunif(x, min=0.01, max=0.03), xlab="x", ylab="",
     xlim=c(0, x.max), type='l')
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 2
sim.means <- replicate(10000, mean(runif(n, min=0.01, max=0.03)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 2700))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 10
sim.means <- replicate(10000, mean(runif(n, min=0.01, max=0.03)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 2700))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 30
sim.means <- replicate(10000, mean(runif(n, min=0.01, max=0.03)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 2700))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```



## Skew vs Sample Size

```{r, echo=FALSE}
n <- 75
sim.means <- replicate(10000, mean(runif(n, min=0.01, max=0.03)))
hist(sim.means, main=paste("n = ", n, sep=""), xlab=expression(bar(x)), xlim=c(0, x.max),
     breaks = breaks, ylim=c(0, 2700))
# abline(v=exp(meanlog + sdlog^2/2), col="red", lwd=2)
```


## Takehome

- At large sample sizes, the Normal approximation to a mean (or a standardized mean) via the CLT becomes useful for finding intervals for populations that are not well-behaved.
- If you have only a small sample from a highly skewed populations, you are likely stuck.
